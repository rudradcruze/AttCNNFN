{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPe07dcxYYNO912AOMf7CM4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EVn8oTNJOAPc","executionInfo":{"status":"ok","timestamp":1720272764927,"user_tz":-360,"elapsed":54868,"user":{"displayName":"CoreByte Solution","userId":"15555132188785320280"}},"outputId":"78d32b28-25c1-4b53-fc6d-b3920cacc36a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.neighbors import KNeighborsClassifier\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from gensim.models import Word2Vec\n","\n","# Download NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Define a function for text preprocessing (stemming and cleaning)\n","def preprocess_text(text):\n","    text = re.sub('[^a-zA-Z]', ' ', text)\n","    text = text.lower()\n","    text = text.split()\n","    ps = PorterStemmer()\n","    text = ' '.join([ps.stem(word) for word in text if not word in set(stopwords.words('english'))])\n","    return text\n","\n","# Load dataset (adjust path as per your file location)\n","news_dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/fakeNewsData.csv')\n","news_dataset = news_dataset.fillna('')\n","\n","# Combine author and title into content\n","news_dataset['content'] = news_dataset['author'] + ' ' + news_dataset['title']\n","news_dataset['content'] = news_dataset['content'].apply(preprocess_text)\n","\n","# Tokenize text for Word2Vec training\n","tokenized_text = [text.split() for text in news_dataset['content']]\n","\n","# Train Word2Vec model\n","wv_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Function to average Word2Vec vectors for a text\n","def word_averaging(wv_model, words):\n","    mean = np.zeros((wv_model.vector_size,))\n","    count = 0.\n","    for word in words:\n","        if word in wv_model.wv:\n","            mean += wv_model.wv[word]\n","            count += 1.\n","    if count != 0:\n","        mean /= count\n","    return mean\n","\n","# Convert each document into average Word2Vec vectors\n","X = np.array([word_averaging(wv_model, words) for words in tokenized_text])\n","y = news_dataset['label']\n","\n","# Split dataset into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train KNN model\n","knn_model = KNeighborsClassifier(n_neighbors=5)\n","knn_model.fit(X_train, y_train)\n","\n","# Predictions\n","y_pred = knn_model.predict(X_test)"]},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\n","\n","# Predictions\n","y_pred = knn_model.predict(X_test)\n","\n","# Confusion Matrix\n","cm = confusion_matrix(y_test, y_pred)\n","print(f\"Confusion Matrix:\\n{cm}\")\n","\n","# Accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy:.4f}\")\n","\n","# Precision\n","precision = precision_score(y_test, y_pred)\n","print(f\"Precision: {precision:.4f}\")\n","\n","# Recall (Sensitivity)\n","recall = recall_score(y_test, y_pred)\n","print(f\"Recall (Sensitivity): {recall:.4f}\")\n","\n","# F1 Score\n","f1 = f1_score(y_test, y_pred)\n","print(f\"F1 Score: {f1:.4f}\")\n","\n","# Matthews Correlation Coefficient (MCC)\n","mcc = matthews_corrcoef(y_test, y_pred)\n","print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n","\n","# AUC Score\n","y_scores = knn_model.predict_proba(X_test)[:, 1]\n","auc_score = roc_auc_score(y_test, y_scores)\n","print(f\"AUC Score: {auc_score:.4f}\")\n","\n","# Calculate Specificity\n","TN = cm[0, 0]  # True Negatives\n","FP = cm[0, 1]  # False Positives\n","specificity = TN / (TN + FP)\n","print(f\"Specificity: {specificity:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3wpAU1LdOO1Q","executionInfo":{"status":"ok","timestamp":1720272902360,"user_tz":-360,"elapsed":2621,"user":{"displayName":"CoreByte Solution","userId":"15555132188785320280"}},"outputId":"c6dc2775-1fe5-4b2d-fd90-5e8ae4f2714d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion Matrix:\n","[[1920  212]\n"," [ 138 1890]]\n","Accuracy: 0.9159\n","Precision: 0.8991\n","Recall (Sensitivity): 0.9320\n","F1 Score: 0.9153\n","Matthews Correlation Coefficient (MCC): 0.8323\n","AUC Score: 0.9603\n","Specificity: 0.9006\n"]}]}]}