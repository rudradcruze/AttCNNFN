{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP8pJqUX4y0hUQO0+95uIYL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jlcP5IcOkp_9","executionInfo":{"status":"ok","timestamp":1720267453032,"user_tz":-360,"elapsed":85835,"user":{"displayName":"CoreByte Solution","userId":"15555132188785320280"}},"outputId":"fbf7d0e1-ffdf-420b-98e6-bc29a63a758a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","GPU PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') available: True\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["\n","Training Dense Network with CountVectorizer Features...\n","Epoch 1/20\n","212/212 [==============================] - 3s 9ms/step - loss: 0.1102 - accuracy: 0.9601 - val_loss: 0.0386 - val_accuracy: 0.9878\n","Epoch 2/20\n","212/212 [==============================] - 1s 6ms/step - loss: 0.0190 - accuracy: 0.9936 - val_loss: 0.0433 - val_accuracy: 0.9880\n","Epoch 3/20\n","212/212 [==============================] - 1s 6ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.0463 - val_accuracy: 0.9885\n","Epoch 4/20\n","212/212 [==============================] - 1s 5ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0558 - val_accuracy: 0.9886\n","Epoch 5/20\n","212/212 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0645 - val_accuracy: 0.9879\n","Epoch 6/20\n","212/212 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.0767 - val_accuracy: 0.9875\n","Epoch 7/20\n","212/212 [==============================] - 2s 8ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0676 - val_accuracy: 0.9874\n","Epoch 8/20\n","212/212 [==============================] - 2s 9ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0625 - val_accuracy: 0.9885\n","Epoch 9/20\n","212/212 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0632 - val_accuracy: 0.9890\n","Epoch 10/20\n","212/212 [==============================] - 1s 5ms/step - loss: 6.8406e-04 - accuracy: 0.9998 - val_loss: 0.0758 - val_accuracy: 0.9891\n","Epoch 11/20\n","212/212 [==============================] - 1s 6ms/step - loss: 9.1283e-04 - accuracy: 0.9997 - val_loss: 0.0785 - val_accuracy: 0.9883\n","Epoch 12/20\n","212/212 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0792 - val_accuracy: 0.9878\n","Epoch 13/20\n","212/212 [==============================] - 1s 6ms/step - loss: 0.0039 - accuracy: 0.9996 - val_loss: 0.0625 - val_accuracy: 0.9889\n","Epoch 14/20\n","212/212 [==============================] - 1s 6ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0681 - val_accuracy: 0.9890\n","Epoch 15/20\n","212/212 [==============================] - 1s 6ms/step - loss: 4.3648e-04 - accuracy: 0.9998 - val_loss: 0.0722 - val_accuracy: 0.9894\n","Epoch 16/20\n","212/212 [==============================] - 1s 6ms/step - loss: 5.0568e-04 - accuracy: 0.9999 - val_loss: 0.0851 - val_accuracy: 0.9887\n","Epoch 17/20\n","212/212 [==============================] - 1s 7ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0787 - val_accuracy: 0.9876\n","Epoch 18/20\n","212/212 [==============================] - 2s 9ms/step - loss: 9.8029e-04 - accuracy: 0.9996 - val_loss: 0.0883 - val_accuracy: 0.9876\n","Epoch 19/20\n","212/212 [==============================] - 2s 7ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0872 - val_accuracy: 0.9876\n","Epoch 20/20\n","212/212 [==============================] - 1s 6ms/step - loss: 1.4350e-04 - accuracy: 1.0000 - val_loss: 0.0938 - val_accuracy: 0.9883\n"]}],"source":["# Mount Google Drive to access files (if needed)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Dropout, Input\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","\n","# Check if GPU is available and use it\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if len(physical_devices) == 0:\n","    print(\"No GPU available. Using CPU instead.\")\n","else:\n","    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","    print(f'GPU {physical_devices[0]} available: True')\n","\n","# Download NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Define a function for text preprocessing (stemming and cleaning)\n","def preprocess_text(text):\n","    text = re.sub('[^a-zA-Z]', ' ', text)\n","    text = text.lower()\n","    text = text.split()\n","    ps = PorterStemmer()\n","    text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n","    text = ' '.join(text)\n","    return text\n","\n","# Load dataset (adjust path as per your file location)\n","news_dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/fakeNewsData.csv')\n","news_dataset = news_dataset.fillna('')\n","\n","# Combine author and title into content\n","news_dataset['content'] = news_dataset['author'] + ' ' + news_dataset['title']\n","news_dataset['content'] = news_dataset['content'].apply(preprocess_text)\n","\n","# Split dataset into train and test sets\n","X = news_dataset['content']\n","y = news_dataset['label']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=1)\n","\n","# Vectorization using CountVectorizer\n","cv = CountVectorizer(max_features=5000)\n","X_train_cv = cv.fit_transform(X_train).toarray()\n","X_test_cv = cv.transform(X_test).toarray()\n","\n","# Define Dense model (removing Conv1D)\n","input_layer = Input(shape=(X_train_cv.shape[1],))  # Shape for CountVectorizer input\n","dense_layer_1 = Dense(128, activation='relu')(input_layer)\n","dropout_layer_1 = Dropout(0.5)(dense_layer_1)\n","dense_layer_2 = Dense(64, activation='relu')(dropout_layer_1)\n","dropout_layer_2 = Dropout(0.5)(dense_layer_2)\n","output_layer = Dense(1, activation='sigmoid')(dropout_layer_2)\n","\n","model = Model(inputs=input_layer, outputs=output_layer)\n","\n","model.compile(optimizer=Adam(learning_rate=0.005), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","print(\"\\nTraining Dense Network with CountVectorizer Features...\")\n","history = model.fit(X_train_cv, y_train, epochs=20, batch_size=64, validation_data=(X_test_cv, y_test), verbose=1)"]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","\n","# Accuracy\n","loss, accuracy = model.evaluate(X_test_cv, y_test)\n","\n","print(f\"\\nTest Accuracy: {accuracy}\")\n","print(f\"\\nLoss: {loss}\")\n","\n","# Predict probabilities for test set\n","y_pred_prob = model.predict(X_test_cv)\n","\n","# Convert probabilities to binary predictions (0 or 1)\n","y_pred = (y_pred_prob > 0.5).astype(int)\n","\n","# Confusion Matrix\n","cm = confusion_matrix(y_test, y_pred)\n","print(f\"Confusion Matrix:\\n{cm}\")\n","\n","# AUC Score\n","auc_score = roc_auc_score(y_test, y_pred_prob)\n","print(f\"AUC Score: {auc_score:.4f}\")\n","\n","# Calculate True Positives, True Negatives, False Positives, False Negatives\n","tn, fp, fn, tp = cm.ravel()\n","\n","# Sensitivity (Recall)\n","sensitivity = tp / (tp + fn)\n","print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n","\n","# Specificity\n","specificity = tn / (tn + fp)\n","print(f\"Specificity: {specificity:.4f}\")\n","\n","# Precision\n","precision = precision_score(y_test, y_pred)\n","print(f\"Precision: {precision:.4f}\")\n","\n","# F1 Score\n","f1 = f1_score(y_test, y_pred)\n","print(f\"F1 Score: {f1:.4f}\")\n","\n","# Matthews Correlation Coefficient (MCC)\n","mcc = matthews_corrcoef(y_test, y_pred)\n","print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0XwVJbpdldpD","executionInfo":{"status":"ok","timestamp":1720267455567,"user_tz":-360,"elapsed":2539,"user":{"displayName":"CoreByte Solution","userId":"15555132188785320280"}},"outputId":"cd47db48-c12a-48da-c352-0b37b2e9e2cf"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["228/228 [==============================] - 1s 3ms/step - loss: 0.0938 - accuracy: 0.9883\n","\n","Test Accuracy: 0.9883241653442383\n","\n","Loss: 0.09376787394285202\n","228/228 [==============================] - 0s 2ms/step\n","Confusion Matrix:\n","[[3567   52]\n"," [  33 3628]]\n","AUC Score: 0.9985\n","Sensitivity (Recall): 0.9910\n","Specificity: 0.9856\n","Precision: 0.9859\n","F1 Score: 0.9884\n","Matthews Correlation Coefficient (MCC): 0.9767\n"]}]}]}