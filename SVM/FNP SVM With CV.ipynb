{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNcOPOpeDT/vX4/ErRFNlpk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uTuyKt19H3o0","executionInfo":{"status":"ok","timestamp":1720271485150,"user_tz":-360,"elapsed":764663,"user":{"displayName":"CoreByte Solution","userId":"15555132188785320280"}},"outputId":"1394cc56-e861-4092-b9ce-7ab654870fa3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","No GPU available. Using CPU instead.\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["# Import necessary libraries\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef, accuracy_score\n","from sklearn.svm import SVC\n","import tensorflow as tf\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","\n","# Mount Google Drive to access files (if needed)\n","drive.mount('/content/drive')\n","\n","# Check if GPU is available and use it\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if len(physical_devices) == 0:\n","    print(\"No GPU available. Using CPU instead.\")\n","else:\n","    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","    print(f'GPU {physical_devices[0]} available: True')\n","\n","# Download NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Define a function for text preprocessing (stemming and cleaning)\n","def preprocess_text(text):\n","    text = re.sub('[^a-zA-Z]', ' ', text)\n","    text = text.lower()\n","    text = text.split()\n","    ps = PorterStemmer()\n","    text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n","    text = ' '.join(text)\n","    return text\n","\n","# Load dataset (adjust path as per your file location)\n","news_dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/fakeNewsData.csv')\n","news_dataset = news_dataset.fillna('')\n","\n","# Combine author and title into content\n","news_dataset['content'] = news_dataset['author'] + ' ' + news_dataset['title']\n","news_dataset['content'] = news_dataset['content'].apply(preprocess_text)\n","\n","# Split dataset into train and test sets\n","X = news_dataset['content']\n","y = news_dataset['label']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Vectorization using CountVectorizer\n","cv = CountVectorizer(max_features=5000)\n","X_train_cv = cv.fit_transform(X_train).toarray()\n","X_test_cv = cv.transform(X_test).toarray()\n","\n","# Train SVM model\n","svm_model = SVC(kernel='rbf', random_state=1)\n","svm_model.fit(X_train_cv, y_train)\n","\n","# Predictions\n","y_pred = svm_model.predict(X_test_cv)"]},{"cell_type":"code","source":["# Calculate evaluation metrics\n","\n","# Confusion Matrix\n","cm = confusion_matrix(y_test, y_pred)\n","print(f\"Confusion Matrix:\\n{cm}\")\n","\n","# AUC Score\n","y_pred_prob = svm_model.decision_function(X_test_cv)\n","auc_score = roc_auc_score(y_test, y_pred_prob)\n","print(f\"AUC Score: {auc_score:.4f}\")\n","\n","# Accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy:.4f}\")\n","\n","# Matthews Correlation Coefficient (MCC)\n","mcc = matthews_corrcoef(y_test, y_pred)\n","print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n","\n","# Sensitivity (Recall)\n","recall = recall_score(y_test, y_pred)\n","print(f\"Sensitivity (Recall): {recall:.4f}\")\n","\n","# Specificity\n","tn, fp, fn, tp = cm.ravel()\n","specificity = tn / (tn + fp)\n","print(f\"Specificity: {specificity:.4f}\")\n","\n","# Precision\n","precision = precision_score(y_test, y_pred)\n","print(f\"Precision: {precision:.4f}\")\n","\n","# F1 Score\n","f1 = f1_score(y_test, y_pred)\n","print(f\"F1 Score: {f1:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FNywUX1LH_P8","executionInfo":{"status":"ok","timestamp":1720271669879,"user_tz":-360,"elapsed":184731,"user":{"displayName":"CoreByte Solution","userId":"15555132188785320280"}},"outputId":"d385933a-7f07-43c0-94bc-ec4bfeeafb11"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion Matrix:\n","[[3078   70]\n"," [  17 3075]]\n","AUC Score: 0.9982\n","Accuracy: 0.9861\n","Matthews Correlation Coefficient (MCC): 0.9723\n","Sensitivity (Recall): 0.9945\n","Specificity: 0.9778\n","Precision: 0.9777\n","F1 Score: 0.9861\n"]}]}]}