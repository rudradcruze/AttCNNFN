{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNjunXN6WQaBqyUROIEx58X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtTK9mZUdWsi","executionInfo":{"status":"ok","timestamp":1720212772987,"user_tz":-360,"elapsed":2474189,"user":{"displayName":"CoreByte Solution","userId":"15555132188785320280"}},"outputId":"75abf203-979c-4584-f79e-ebc1fc58c8ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","No GPU available. Using CPU instead.\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["\n","Training CNN with Attention and Word2Vec Features...\n","Epoch 1/20\n","520/520 [==============================] - 118s 224ms/step - loss: 0.3544 - accuracy: 0.8477 - val_loss: 0.2225 - val_accuracy: 0.9183\n","Epoch 2/20\n","520/520 [==============================] - 116s 223ms/step - loss: 0.2057 - accuracy: 0.9272 - val_loss: 0.1845 - val_accuracy: 0.9370\n","Epoch 3/20\n","520/520 [==============================] - 125s 239ms/step - loss: 0.1777 - accuracy: 0.9404 - val_loss: 0.1774 - val_accuracy: 0.9368\n","Epoch 4/20\n","520/520 [==============================] - 115s 221ms/step - loss: 0.1651 - accuracy: 0.9434 - val_loss: 0.1707 - val_accuracy: 0.9394\n","Epoch 5/20\n","520/520 [==============================] - 125s 240ms/step - loss: 0.1569 - accuracy: 0.9460 - val_loss: 0.1716 - val_accuracy: 0.9385\n","Epoch 6/20\n","520/520 [==============================] - 119s 229ms/step - loss: 0.1522 - accuracy: 0.9472 - val_loss: 0.1594 - val_accuracy: 0.9438\n","Epoch 7/20\n","520/520 [==============================] - 115s 220ms/step - loss: 0.1464 - accuracy: 0.9481 - val_loss: 0.1499 - val_accuracy: 0.9469\n","Epoch 8/20\n","520/520 [==============================] - 117s 225ms/step - loss: 0.1409 - accuracy: 0.9524 - val_loss: 0.1593 - val_accuracy: 0.9435\n","Epoch 9/20\n","520/520 [==============================] - 115s 221ms/step - loss: 0.1375 - accuracy: 0.9528 - val_loss: 0.1430 - val_accuracy: 0.9488\n","Epoch 10/20\n","520/520 [==============================] - 125s 241ms/step - loss: 0.1323 - accuracy: 0.9541 - val_loss: 0.1617 - val_accuracy: 0.9474\n","Epoch 11/20\n","520/520 [==============================] - 120s 231ms/step - loss: 0.1287 - accuracy: 0.9553 - val_loss: 0.1545 - val_accuracy: 0.9495\n","Epoch 12/20\n","520/520 [==============================] - 124s 238ms/step - loss: 0.1250 - accuracy: 0.9575 - val_loss: 0.1556 - val_accuracy: 0.9469\n","Epoch 13/20\n","520/520 [==============================] - 125s 241ms/step - loss: 0.1263 - accuracy: 0.9564 - val_loss: 0.1517 - val_accuracy: 0.9481\n","Epoch 14/20\n","520/520 [==============================] - 115s 221ms/step - loss: 0.1200 - accuracy: 0.9590 - val_loss: 0.1363 - val_accuracy: 0.9541\n","Epoch 15/20\n","520/520 [==============================] - 124s 238ms/step - loss: 0.1173 - accuracy: 0.9599 - val_loss: 0.1335 - val_accuracy: 0.9548\n","Epoch 16/20\n","520/520 [==============================] - 115s 221ms/step - loss: 0.1185 - accuracy: 0.9590 - val_loss: 0.1438 - val_accuracy: 0.9538\n","Epoch 17/20\n","520/520 [==============================] - 125s 240ms/step - loss: 0.1181 - accuracy: 0.9569 - val_loss: 0.1410 - val_accuracy: 0.9531\n","Epoch 18/20\n","520/520 [==============================] - 115s 222ms/step - loss: 0.1103 - accuracy: 0.9615 - val_loss: 0.1280 - val_accuracy: 0.9555\n","Epoch 19/20\n","520/520 [==============================] - 125s 240ms/step - loss: 0.1115 - accuracy: 0.9616 - val_loss: 0.1441 - val_accuracy: 0.9526\n","Epoch 20/20\n","520/520 [==============================] - 114s 220ms/step - loss: 0.1064 - accuracy: 0.9630 - val_loss: 0.1323 - val_accuracy: 0.9579\n"]}],"source":["# Mount Google Drive to access files (if needed)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import re\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Dropout, Input, Attention\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from gensim.models import Word2Vec\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","\n","# Check if GPU is available and use it\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if len(physical_devices) == 0:\n","    print(\"No GPU available. Using CPU instead.\")\n","else:\n","    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","    print(f'GPU {physical_devices[0]} available: True')\n","\n","# Download NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Define a function for text preprocessing (stemming and cleaning)\n","def preprocess_text(text):\n","    text = re.sub('[^a-zA-Z]', ' ', text)\n","    text = text.lower()\n","    text = text.split()\n","    ps = PorterStemmer()\n","    text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n","    text = ' '.join(text)\n","    return text\n","\n","# Load dataset (adjust path as per your file location)\n","news_dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/fakeNewsData.csv')\n","news_dataset = news_dataset.fillna('')\n","\n","# Combine author and title into content\n","news_dataset['content'] = news_dataset['author'] + ' ' + news_dataset['title']\n","news_dataset['content'] = news_dataset['content'].apply(preprocess_text)\n","\n","# Split dataset into train and test sets\n","X = news_dataset['content']\n","y = news_dataset['label']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train Word2Vec model\n","sentences = [text.split() for text in X_train]\n","wv_model = Word2Vec(sentences=sentences, vector_size=200, window=5, min_count=2, workers=4)\n","\n","# Function to get Word2Vec embeddings\n","def get_word2vec_embeddings(wv_model, texts, max_len):\n","    embeddings = np.zeros((len(texts), max_len, wv_model.vector_size))\n","    for i, text in enumerate(texts):\n","        words = text.split()\n","        for j, word in enumerate(words):\n","            if j == max_len:\n","                break\n","            if word in wv_model.wv:\n","                embeddings[i, j] = wv_model.wv[word]\n","    return embeddings\n","\n","max_len = 200  # Adjusted max length to manage memory usage\n","X_train_word2vec = get_word2vec_embeddings(wv_model, X_train, max_len)\n","X_test_word2vec = get_word2vec_embeddings(wv_model, X_test, max_len)\n","\n","# Define CNN with Attention\n","input_layer = Input(shape=(X_train_word2vec.shape[1], X_train_word2vec.shape[2]))\n","conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(input_layer)\n","attention_data = Conv1D(filters=128, kernel_size=3, activation='relu')(input_layer)\n","attention_layer = Attention()([conv_layer, attention_data])\n","flatten_layer = GlobalMaxPooling1D()(attention_layer)\n","dropout_layer = Dropout(0.5)(flatten_layer)\n","dense_layer_1 = Dense(64, activation='relu')(dropout_layer)\n","output_layer = Dense(1, activation='sigmoid')(dense_layer_1)\n","\n","model_attention = Model(inputs=input_layer, outputs=output_layer)\n","\n","model_attention.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","print(\"\\nTraining CNN with Attention and Word2Vec Features...\")\n","history_attention = model_attention.fit(X_train_word2vec, y_train, epochs=20, batch_size=32, validation_data=(X_test_word2vec, y_test), verbose=1)\n"]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","\n","# Accuracy\n","loss_attention, accuracy_attention = model_attention.evaluate(X_test_word2vec, y_test)\n","\n","print(f\"\\nAttention CNN Test Accuracy: {accuracy_attention}\")\n","print(f\"\\n Loss Attention: {loss_attention}\")\n","\n","# Predict probabilities for test set\n","y_pred_prob = model_attention.predict(X_test_word2vec)\n","\n","# Convert probabilities to binary predictions (0 or 1)\n","y_pred = (y_pred_prob > 0.5).astype(int)\n","\n","# Confusion Matrix\n","cm = confusion_matrix(y_test, y_pred)\n","print(f\"Confusion Matrix:\\n{cm}\")\n","\n","# AUC Score\n","auc_score = roc_auc_score(y_test, y_pred_prob)\n","print(f\"AUC Score: {auc_score:.4f}\")\n","\n","# Calculate True Positives, True Negatives, False Positives, False Negatives\n","tn, fp, fn, tp = cm.ravel()\n","\n","# Sensitivity (Recall)\n","sensitivity = tp / (tp + fn)\n","print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n","\n","# Specificity\n","specificity = tn / (tn + fp)\n","print(f\"Specificity: {specificity:.4f}\")\n","\n","# Precision\n","precision = precision_score(y_test, y_pred)\n","print(f\"Precision: {precision:.4f}\")\n","\n","# F1 Score\n","f1 = f1_score(y_test, y_pred)\n","print(f\"F1 Score: {f1:.4f}\")\n","\n","# Matthews Correlation Coefficient (MCC)\n","mcc = matthews_corrcoef(y_test, y_pred)\n","print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WutEcJPRdudk","executionInfo":{"status":"ok","timestamp":1720212798749,"user_tz":-360,"elapsed":25781,"user":{"displayName":"CoreByte Solution","userId":"15555132188785320280"}},"outputId":"59ae2aa5-2218-48f2-b9f7-db612543faa7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["130/130 [==============================] - 10s 74ms/step - loss: 0.1323 - accuracy: 0.9579\n","\n","Attention CNN Test Accuracy: 0.957932710647583\n","\n"," Loss Attention: 0.1323414295911789\n","130/130 [==============================] - 11s 81ms/step\n","Confusion Matrix:\n","[[1980  152]\n"," [  23 2005]]\n","AUC Score: 0.9864\n","Sensitivity (Recall): 0.9887\n","Specificity: 0.9287\n","Precision: 0.9295\n","F1 Score: 0.9582\n","Matthews Correlation Coefficient (MCC): 0.9177\n"]}]}]}